{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12ddd9a-a194-430a-ac5f-40ea5de7c914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "🚀 StorytellerGPT Tokenizer Training: PHASE 4 INIT\n",
      "🕒 Started at: 2025-07-29 09:57:27.017089\n",
      "============================\n",
      "\n",
      "✅ Corpus File: C:/Kishan/training_data/train_bulk_txt/merged_train.txt\n",
      "✅ Custom Tokens: C:/Kishan/training_data/custom_tokens.txt\n",
      "\n",
      "🧿 Loaded 40 custom tags:\n",
      "   → ['#chapter_start', '#chapter_end', '#dialogue', '#quote', '#poem'] ...\n",
      "\n",
      "📜 SentencePiece Training Command Constructed:\n",
      "\n",
      "--input=C:/Kishan/training_data/train_bulk_txt/merged_train.txt --model_prefix=C:/Kishan/training_data/tokenizer/tokenizer_exp --vocab_size=50000 --model_type=unigram --character_coverage=1.0 --normalization_rule_name=identity --user_defined_symbols=#chapter_start,#chapter_end,#dialogue,#quote,#poem,#story_within,#character,#title,#hero,#villain,#mentor,#outsider,#noble_house,#place,#weapon,#artifact,#creature,#god,#prophecy,#magic,#curse,#curse_break,#ritual,#vision,#transformation,#battle,#trial,#betrayal,#rebellion,#alliance,#oath,#lineage,#legacy,#festival,#law,#death,#funeral,#editor_note,#annotation,#translation_note --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --train_extremely_large_corpus=true --input_sentence_size=10000000 --shuffle_input_sentence=true --hard_vocab_limit=false --unk_surface=<unk> --max_sentence_length=2048 --num_threads=12\n",
      "\n",
      "💥 Launching Training... Hold tight!\n",
      "\n",
      "\n",
      "🎉 Tokenizer Training Complete!\n",
      "🕒 Duration: 15.7 minutes\n",
      "📦 Model: C:/Kishan/training_data/tokenizer/tokenizer_exp.model\n",
      "📋 Vocab: C:/Kishan/training_data/tokenizer/tokenizer_exp.vocab\n",
      "\n",
      "🔎 Running Post-Training Validation...\n",
      "\n",
      "🧪 Tags Found Intact: 40/40\n",
      "🎯 All tags preserved intact as atomic units!\n",
      "📓 Full training log saved to: C:/Kishan/training_data/tokenizer/tokenizer_exp_train_log.txt\n",
      "\n",
      "🌟 PHASE 4 COMPLETE — Tokenizer is now READY for Phase 5 binarization!\n",
      "🏁 TRAINING COMPLETE. GO SHOW DEEPSEEK WHO BUILT THE REAL SHIT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ FINAL FIXED SCRIPT — SentencePiece Tokenizer Trainer for StorytellerGPT\n",
    "# ⚠️ TRAINING COMMAND PASSED AS STRING (not multiline)\n",
    "\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# === CONFIG ===\n",
    "CORPUS_PATH = \"C:/Kishan/training_data/train_bulk_txt/merged_train.txt\"\n",
    "CUSTOM_TOKEN_FILE = \"C:/Kishan/training_data/custom_tokens.txt\"\n",
    "MODEL_PREFIX = \"C:/Kishan/training_data/tokenizer/tokenizer_exp\"\n",
    "VOCAB_SIZE = 50000\n",
    "LOG_FILE = MODEL_PREFIX + \"_train_log.txt\"\n",
    "MODEL_TYPE = \"unigram\"\n",
    "\n",
    "# === PRINT HEADER ===\n",
    "print(\"\\n============================\")\n",
    "print(\"\\U0001F680 StorytellerGPT Tokenizer Training: PHASE 4 INIT\")\n",
    "print(f\"\\U0001F552 Started at: {datetime.now()}\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "# === CHECKS ===\n",
    "assert os.path.exists(CORPUS_PATH), f\"❌ Corpus file not found at: {CORPUS_PATH}\"\n",
    "assert os.path.exists(CUSTOM_TOKEN_FILE), f\"❌ custom_tokens.txt missing at: {CUSTOM_TOKEN_FILE}\"\n",
    "print(f\"✅ Corpus File: {CORPUS_PATH}\")\n",
    "print(f\"✅ Custom Tokens: {CUSTOM_TOKEN_FILE}\\n\")\n",
    "\n",
    "# === LOAD CUSTOM TOKENS ===\n",
    "with open(CUSTOM_TOKEN_FILE, 'r', encoding='utf-8') as f:\n",
    "    tokens = [line.strip() for line in f if line.strip() and line.startswith('#')]\n",
    "user_defined_symbols = \",\".join(tokens)\n",
    "print(f\"\\U0001F9FF Loaded {len(tokens)} custom tags:\")\n",
    "print(\"   →\", tokens[:5], \"...\\n\")\n",
    "\n",
    "# === FIXED: SINGLE-LINE TRAINING COMMAND ===\n",
    "spm_cmd = (\n",
    "    f\"--input={CORPUS_PATH} \"\n",
    "    f\"--model_prefix={MODEL_PREFIX} \"\n",
    "    f\"--vocab_size={VOCAB_SIZE} \"\n",
    "    f\"--model_type={MODEL_TYPE} \"\n",
    "    \"--character_coverage=1.0 \"\n",
    "    \"--normalization_rule_name=identity \"\n",
    "    f\"--user_defined_symbols={user_defined_symbols} \"\n",
    "    \"--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 \"\n",
    "    \"--train_extremely_large_corpus=true \"\n",
    "    \"--input_sentence_size=10000000 \"\n",
    "    \"--shuffle_input_sentence=true \"\n",
    "    \"--hard_vocab_limit=false \"\n",
    "    \"--unk_surface=<unk> \"\n",
    "    \"--max_sentence_length=2048 \"\n",
    "    \"--num_threads=12\"\n",
    ")\n",
    "\n",
    "print(\"\\U0001F4DC SentencePiece Training Command Constructed:\\n\")\n",
    "print(spm_cmd)\n",
    "print(\"\\n\\U0001F4A5 Launching Training... Hold tight!\\n\")\n",
    "# === TRAIN ===\n",
    "start_time = time.time()\n",
    "spm.SentencePieceTrainer.Train(spm_cmd)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\\U0001F389 Tokenizer Training Complete!\")\n",
    "print(f\"\\U0001F552 Duration: {round((end_time - start_time) / 60, 2)} minutes\")\n",
    "print(f\"\\U0001F4E6 Model: {MODEL_PREFIX}.model\")\n",
    "print(f\"\\U0001F4CB Vocab: {MODEL_PREFIX}.vocab\\n\")\n",
    "\n",
    "# === POST-VALIDATION ===\n",
    "print(\"\\U0001F50E Running Post-Training Validation...\\n\")\n",
    "with open(f\"{MODEL_PREFIX}.vocab\", \"r\", encoding='utf-8') as vocab_file:\n",
    "    vocab_lines = vocab_file.readlines()\n",
    "\n",
    "found_tags = [tag for tag in tokens if any(tag in line for line in vocab_lines)]\n",
    "broken_tags = [tag for tag in tokens if tag not in found_tags]\n",
    "\n",
    "print(f\"\\U0001F9EA Tags Found Intact: {len(found_tags)}/{len(tokens)}\")\n",
    "if broken_tags:\n",
    "    print(f\"❌ Tags Possibly Broken: {broken_tags[:5]} ...\")\n",
    "else:\n",
    "    print(\"\\U0001F3AF All tags preserved intact as atomic units!\")\n",
    "\n",
    "# === LOGGING ===\n",
    "with open(LOG_FILE, 'w', encoding='utf-8') as log:\n",
    "    log.write(f\"Training Log - {datetime.now()}\\n\")\n",
    "    log.write(f\"Corpus: {CORPUS_PATH}\\n\")\n",
    "    log.write(f\"Vocab Size: {VOCAB_SIZE}\\n\")\n",
    "    log.write(f\"Custom Tags Preserved: {len(found_tags)}\\n\")\n",
    "    log.write(f\"Model Output: {MODEL_PREFIX}.model\\n\")\n",
    "    log.write(f\"Duration: {round((end_time - start_time) / 60, 2)} min\\n\")\n",
    "    if broken_tags:\n",
    "        log.write(f\"Broken Tags: {broken_tags}\\n\")\n",
    "    else:\n",
    "        log.write(\"All tags preserved successfully.\\n\")\n",
    "\n",
    "print(\"\\U0001F4D3 Full training log saved to:\", LOG_FILE)\n",
    "print(\"\\n\\U0001F31F PHASE 4 COMPLETE — Tokenizer is now READY for Phase 5 binarization!\")\n",
    "print(\"\\U0001F3C1 TRAINING COMPLETE. GO SHOW DEEPSEEK WHO BUILT THE REAL SHIT.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16101472-6fe2-4a69-91a3-315bb46d69b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7375e4-1850-40b3-b04b-86ec7564eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔍 TOKENIZER VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "📜 Original: #chapter_start The tale of two brothers begins...\n",
      "🧩 Pieces  : ['▁', '#chapter_start', '▁The', '▁tale', '▁of', '▁two', '▁brothers', '▁begins', '.', '.', '.']\n",
      "🔢 IDs     : [105, 4, 73, 1929, 47, 164, 1921, 3801, 46, 46]...\n",
      "🔁 Decoded : #chapter_start The tale of two brothers begins...\n",
      "\n",
      "📜 Original: He unsheathed the #weapon under moonlight.\n",
      "🧩 Pieces  : ['▁He', '▁uns', 'heath', 'ed', '▁the', '▁', '#weapon', '▁under', '▁moonlight', '.']\n",
      "🔢 IDs     : [108, 5501, 32543, 141, 45, 105, 18, 222, 5418, 46]...\n",
      "🔁 Decoded : He unsheathed the #weapon under moonlight.\n",
      "\n",
      "📜 Original: #magic was forbidden in the land of #creature and #god.\n",
      "🧩 Pieces  : ['▁', '#magic', '▁was', '▁forbidden', '▁in', '▁the', '▁land', '▁of', '▁', '#creature', '▁and', '▁', '#god', '.']\n",
      "🔢 IDs     : [105, 23, 55, 6707, 52, 45, 523, 47, 105, 20]...\n",
      "🔁 Decoded : #magic was forbidden in the land of #creature and #god.\n",
      "\n",
      "📜 Original: #dialogue 'I never asked for this fate,' she said.\n",
      "🧩 Pieces  : ['▁', '#dialogue', \"▁'\", 'I', '▁never', '▁asked', '▁for', '▁this', '▁fate', ',', \"'\", '▁she', '▁said', '.']\n",
      "🔢 IDs     : [105, 6, 134, 129, 210, 357, 65, 79, 1695, 44]...\n",
      "🔁 Decoded : #dialogue 'I never asked for this fate,' she said.\n",
      "\n",
      "📜 Original: #curse or prophecy? The lines were blurred.\n",
      "🧩 Pieces  : ['▁', '#curse', '▁or', '▁prophecy', '?', '▁The', '▁lines', '▁were', '▁blurred', '.']\n",
      "🔢 IDs     : [105, 24, 77, 8564, 128, 73, 1198, 86, 25956, 46]...\n",
      "🔁 Decoded : #curse or prophecy? The lines were blurred.\n",
      "\n",
      "📜 Original: Random gibberish Æß¢≠∑™✓ to check unknown handling.\n",
      "🧩 Pieces  : ['▁Ran', 'dom', '▁gi', 'bber', 'ish', '▁Æ', 'ß', '¢', '≠', '∑', '™✓', '▁to', '▁check', '▁unknown', '▁handling', '.']\n",
      "🔢 IDs     : [10935, 5696, 5963, 8764, 1651, 7965, 1, 39973, 1, 47285]...\n",
      "🔁 Decoded : Random gibberish Æ<unk>¢<unk>∑<unk> to check unknown handling.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load your trained tokenizer model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"C:/Kishan/training_data/tokenizer/tokenizer_exp.model\")\n",
    "\n",
    "# Test samples from your actual corpus themes\n",
    "samples = [\n",
    "    \"#chapter_start The tale of two brothers begins...\",\n",
    "    \"He unsheathed the #weapon under moonlight.\",\n",
    "    \"#magic was forbidden in the land of #creature and #god.\",\n",
    "    \"#dialogue 'I never asked for this fate,' she said.\",\n",
    "    \"#curse or prophecy? The lines were blurred.\",\n",
    "    \"Random gibberish Æß¢≠∑™✓ to check unknown handling.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔍 TOKENIZER VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "for text in samples:\n",
    "    pieces = sp.encode_as_pieces(text)\n",
    "    ids = sp.encode_as_ids(text)\n",
    "    decoded = sp.decode_ids(ids)\n",
    "    \n",
    "    print(f\"\\n📜 Original: {text}\")\n",
    "    print(f\"🧩 Pieces  : {pieces}\")\n",
    "    print(f\"🔢 IDs     : {ids[:10]}...\")  # show partial for brevity\n",
    "    print(f\"🔁 Decoded : {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf1c8b-62dd-4028-9ab9-2f1ac4930a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (AiProject)",
   "language": "python",
   "name": "aiproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
