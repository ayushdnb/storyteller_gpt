{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38db643-e470-464d-8af5-0b9c8603af4e",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f6b6c86-bccb-451c-a0b1-21897dc28bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reading: C:\\Kishan\training_data\final_bin\train.bin\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Kishan\\training_data\\x0cinal_bin\\train.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m crc & \u001b[32m0xFFFFFFFF\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     actual_crc = \u001b[43mcompute_crc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBIN_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Computed CRC : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mhex\u001b[39m(actual_crc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîê Expected CRC : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mhex\u001b[39m(EXPECTED_CRC)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcompute_crc\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_crc\u001b[39m(file_path: Path) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîç Reading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     13\u001b[39m         crc = \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk := f.read(\u001b[32m8192\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\AiProject\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: 'C:\\\\Kishan\\training_data\\x0cinal_bin\\train.bin'"
     ]
    }
   ],
   "source": [
    "# üîç crc_verify.py\n",
    "import zlib\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== CONFIG ====\n",
    "BIN_PATH = Path(r\"C:\\Kishan\\training_data\\final_bin\\train.bin\")  # <- Update if needed\n",
    "EXPECTED_CRC = 0x9A184523      # <- From log\n",
    "DTYPE = \"uint16\"\n",
    "\n",
    "def compute_crc(file_path: Path) -> int:\n",
    "    print(f\"üîç Reading: {file_path}\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        crc = 0\n",
    "        while chunk := f.read(8192):\n",
    "            crc = zlib.crc32(chunk, crc)\n",
    "    return crc & 0xFFFFFFFF\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actual_crc = compute_crc(BIN_PATH)\n",
    "    print(f\"‚úÖ Computed CRC : {hex(actual_crc)}\")\n",
    "    print(f\"üîê Expected CRC : {hex(EXPECTED_CRC)}\")\n",
    "    if actual_crc == EXPECTED_CRC:\n",
    "        print(\"üéØ CRC MATCH ‚Äî File is good.\")\n",
    "    else:\n",
    "        print(\"‚ùå CRC MISMATCH ‚Äî File may be corrupted or incomplete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff169b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Full BIN Integrity + Token Scan\n",
      "\n",
      "üîç Scanning CRC for train.bin...\n",
      "   ‚Üí Expected CRC: 0x9a184523\n",
      "   ‚Üí Actual CRC  : 0x6520e4c3\n",
      "   ‚ùå CRC MISMATCH for train.bin\n",
      "üî¢ Loading tokens from train.bin [1672541 KB]...\n",
      "   üìä Token Count        : 856,341,217\n",
      "   üî£ Unique Token IDs   : 49,449\n",
      "   üìÑ Saved top token stats ‚Üí train.bin_top_tokens.csv\n",
      "   üîç Analyzing CUSTOM TAG frequency in vocab...\n",
      "   ‚úÖ Saved ‚Üí train.bin_custom_token_hits.csv\n",
      "\n",
      "üîç Scanning CRC for val.bin...\n",
      "   ‚Üí Expected CRC: 0x0\n",
      "   ‚Üí Actual CRC  : 0x61009985\n",
      "   ‚ùå CRC MISMATCH for val.bin\n",
      "üî¢ Loading tokens from val.bin [153970 KB]...\n",
      "   üìä Token Count        : 78,832,917\n",
      "   üî£ Unique Token IDs   : 46,912\n",
      "   üìÑ Saved top token stats ‚Üí val.bin_top_tokens.csv\n",
      "   üîç Analyzing CUSTOM TAG frequency in vocab...\n",
      "   ‚úÖ Saved ‚Üí val.bin_custom_token_hits.csv\n",
      "\n",
      "‚úÖ Done in 41.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° Ultra-Optimized BIN Verifier & Token Density Analyzer\n",
    "# By Cypher | For RTX 3060 + i7 + 16GB RAM | StorytellerGPT\n",
    "\n",
    "import zlib, os, time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "BIN_DIR = Path(r\"C:\\Kishan\\training_data\\final_bin\")\n",
    "FILES = {\n",
    "    \"train.bin\": 0x9A184523,  # Replace with actual CRC if needed\n",
    "    \"val.bin\":   0x00000000,  # Replace with val CRC if known\n",
    "}\n",
    "DTYPE = np.uint16  # Adjust if you're using uint8/uint32\n",
    "CUSTOM_TOKENS = [\n",
    "    '#chapter_start', '#chapter_end', '#dialogue', '#quote', '#poem', '#story_within',\n",
    "    '#character', '#title', '#hero', '#villain', '#mentor', '#outsider', '#noble_house',\n",
    "    '#place', '#weapon', '#artifact', '#creature', '#god', '#prophecy', '#magic',\n",
    "    '#curse', '#curse_break', '#ritual', '#vision', '#transformation', '#battle',\n",
    "    '#trial', '#betrayal', '#rebellion', '#alliance', '#oath', '#lineage', '#legacy',\n",
    "    '#festival', '#law', '#death', '#funeral', '#editor_note', '#annotation', '#translation_note'\n",
    "]\n",
    "\n",
    "# === CRC CHECK ===\n",
    "def compute_crc(path: Path) -> int:\n",
    "    print(f\"\\nüîç Scanning CRC for {path.name}...\")\n",
    "    crc = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        while chunk := f.read(8192):\n",
    "            crc = zlib.crc32(chunk, crc)\n",
    "    return crc & 0xFFFFFFFF\n",
    "\n",
    "# === TOKEN STATS ===\n",
    "def analyze_bin(path: Path) -> dict:\n",
    "    print(f\"üî¢ Loading tokens from {path.name} [{path.stat().st_size // 1024} KB]...\", flush=True)\n",
    "    arr = np.fromfile(path, dtype=DTYPE)\n",
    "    total_tokens = len(arr)\n",
    "    unique_tokens, counts = np.unique(arr, return_counts=True)\n",
    "    token_freq = dict(zip(unique_tokens, counts))\n",
    "    return {\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"unique_tokens\": len(unique_tokens),\n",
    "        \"token_freq\": token_freq,\n",
    "        \"np_array\": arr\n",
    "    }\n",
    "\n",
    "# === CUSTOM TOKEN HIT CHECK ===\n",
    "def load_vocab_txt(vocab_path: Path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            token = line.strip().split('\\t')[0]\n",
    "            vocab[token] = i\n",
    "    return vocab\n",
    "\n",
    "def custom_token_stats(vocab_txt: Path, token_freq: dict):\n",
    "    vocab = load_vocab_txt(vocab_txt)\n",
    "    data = []\n",
    "    for tok in CUSTOM_TOKENS:\n",
    "        idx = vocab.get(tok)\n",
    "        count = token_freq.get(idx, 0) if idx is not None else 0\n",
    "        data.append((tok, idx, count))\n",
    "    return pd.DataFrame(data, columns=[\"Token\", \"ID\", \"Count\"]).sort_values(\"Count\", ascending=False)\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Full BIN Integrity + Token Scan\")\n",
    "    start = time.time()\n",
    "\n",
    "    for file, expected_crc in FILES.items():\n",
    "        bin_path = BIN_DIR / file\n",
    "        if not bin_path.exists():\n",
    "            print(f\"‚ùå File not found: {file}\")\n",
    "            continue\n",
    "\n",
    "        crc = compute_crc(bin_path)\n",
    "        print(f\"   ‚Üí Expected CRC: {hex(expected_crc)}\")\n",
    "        print(f\"   ‚Üí Actual CRC  : {hex(crc)}\")\n",
    "        if crc == expected_crc:\n",
    "            print(f\"   ‚úÖ CRC MATCH for {file}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå CRC MISMATCH for {file}\")\n",
    "\n",
    "        stats = analyze_bin(bin_path)\n",
    "        print(f\"   üìä Token Count        : {stats['total_tokens']:,}\")\n",
    "        print(f\"   üî£ Unique Token IDs   : {stats['unique_tokens']:,}\")\n",
    "\n",
    "        # Save token frequency distribution (top 50)\n",
    "        top_tokens = sorted(stats['token_freq'].items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "        df_top = pd.DataFrame(top_tokens, columns=['TokenID', 'Frequency'])\n",
    "        df_top.to_csv(BIN_DIR / f\"{file}_top_tokens.csv\", index=False)\n",
    "        print(f\"   üìÑ Saved top token stats ‚Üí {file}_top_tokens.csv\")\n",
    "\n",
    "        # Check for special token usage\n",
    "        vocab_txt_path = BIN_DIR.parent / \"tokenizer\" / \"tokenizer_exp.vocab\"\n",
    "        if vocab_txt_path.exists():\n",
    "            print(f\"   üîç Analyzing CUSTOM TAG frequency in vocab...\")\n",
    "            df_custom = custom_token_stats(vocab_txt_path, stats['token_freq'])\n",
    "            df_custom.to_csv(BIN_DIR / f\"{file}_custom_token_hits.csv\", index=False)\n",
    "            print(f\"   ‚úÖ Saved ‚Üí {file}_custom_token_hits.csv\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  tokenizer_exp.vocab not found. Skipping custom tag scan.\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Done in {time.time() - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5351d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (AiProject)",
   "language": "python",
   "name": "aiproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
